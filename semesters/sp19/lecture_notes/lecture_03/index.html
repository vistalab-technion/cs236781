<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Lecture 3: Multi-layer Perceptron | CS236781: Deep Learning</title>
<meta name="description" content="MLP, Backpropagation, Gradient Descent, CNNs">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="CS236781: Deep Learning">
<meta property="og:title" content="Lecture 3: Multi-layer Perceptron">
<meta property="og:url" content="https://vistalab-technion.github.io/cs236781/semesters/sp19/lecture_notes/lecture_03/">


  <meta property="og:description" content="MLP, Backpropagation, Gradient Descent, CNNs">











  

  


<link rel="canonical" href="https://vistalab-technion.github.io/cs236781/semesters/sp19/lecture_notes/lecture_03/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "VISTA Lab",
      "url": "https://vistalab-technion.github.iocs236781/semesters/sp19",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/cs236781/semesters/sp19/feed.xml" type="application/atom+xml" rel="alternate" title="CS236781: Deep Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/cs236781/semesters/sp19/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single text-justify wide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/cs236781/semesters/sp19/">CS236781: Deep Learning</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/cs236781/semesters/sp19/info/" >Info</a>
            </li><li class="masthead__menu-item">
              <a href="/cs236781/semesters/sp19/lectures/" >Lectures</a>
            </li><li class="masthead__menu-item">
              <a href="/cs236781/semesters/sp19/tutorials/" >Tutorials</a>
            </li><li class="masthead__menu-item">
              <a href="/cs236781/semesters/sp19/assignments/" >Assignments</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Lecture 3: Multi-layer Perceptron">
    <meta itemprop="description" content="MLP, Backpropagation, Gradient Descent, CNNs">
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Lecture 3: Multi-layer Perceptron
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  56 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Contents</h4></header>
              <ul class="toc__menu">
  <li><a href="#the-perceptron">The Perceptron</a>
    <ul>
      <li><a href="#adding-layers">Adding layers</a></li>
      <li><a href="#non-linearity">Non-linearity</a></li>
    </ul>
  </li>
  <li><a href="#supervised-training">Supervised training</a>
    <ul>
      <li><a href="#global-and-local-minima">Global and local minima</a></li>
      <li><a href="#gradient-descent">Gradient descent</a></li>
      <li><a href="#error-backpropagation">Error backpropagation</a>
        <ul>
          <li><a href="#forward-pass">Forward pass:</a></li>
          <li><a href="#backward-pass">Backward pass:</a></li>
        </ul>
      </li>
      <li><a href="#exploding-and-vanishing-gradients">Exploding and vanishing gradients</a></li>
    </ul>
  </li>
  <li><a href="#convolutional-neural-networks">Convolutional neural networks</a>
    <ul>
      <li><a href="#weight-sharing-and-shift-invariance">Weight sharing and shift invariance</a></li>
      <li><a href="#toeplitz-operators-and-convolution">Toeplitz operators and convolution</a></li>
      <li><a href="#convolutional-layer">Convolutional layer</a></li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <h2 id="the-perceptron">The Perceptron</h2>

<p>In the last lecture, we discussed supervised learning with a linear hypothesis
class of the form</p>

<script type="math/tex; mode=display">y = \bb{w}^\Tr \bb{x}+b</script>

<p>parametrized by $n$ weights $\bb{w} = (w_0,w_1,\dots,w_n)$ and a
bias $b$. In the machine learning literature, this family of functions (or
“architecture” as we shall call it in the sequel) is known as a (linear)
<em>perceptron</em>.</p>

<p>We have seen that in the case of binary logistic regression (which, despite the
name, is a binary <em>classification</em> problem) the scalar output $y$ of the
hypothesis was further fed into the logistic (a.k.a.  <em>sigmoid</em> function</p>

<script type="math/tex; mode=display">\psi(t) = \frac{e^t}{1+e^t}.</script>

<p>This can be viewed as a <em>two-dimensional</em> output of the form</p>

<script type="math/tex; mode=display">\bb{y} = \left(  \frac{e^{ \bb{w}^\Tr \bb{x}+b} }{1+e^{ \bb{w}^\Tr \bb{x} + b}}, \frac{1}{1+e^{ \bb{w}^\Tr \bb{x}+b}} \right)</script>

<p>which can be interpreted as the vector of probabilities of the instance
$\bb{x}$ belonging to each of the two classes.</p>

<p>Using this perspective, the linear perceptron model can be generalized
to the $k$ class cases according to</p>

<script type="math/tex; mode=display">\bb{y} = \frac{ e^{ \bb{W}^\Tr
\bb{x}+\bb{b} }  }{ \bb{1}^\Tr e^{
\bb{W}^\Tr \bb{x} + \bb{b}}  } =
\displaystyle{\left(  \frac{ e^{ \bb{w}_1^\Tr \bb{x} + b_1
} }{\sum_{i=1}^k e^{ \bb{w}_i^\Tr \bb{x} + b_i  } } ,
\dots,  \frac{ e^{ \bb{w}_n^\Tr \bb{x}  + b_n }
}{\sum_{i=1}^k e^{ \bb{w}_i^\Tr \bb{x} + b_i  } }  \right)
},</script>

<p>where $\bb{W}$ is a $k \times n$ weight matrix whose rows are
denoted as $\bb{w}_i$, $\bb{b}$ is a
$k$-dimensional bias vector, and $\bb{1}$ is an
appropriately-sized vector of ones. This generalization of the logistic
function used to normalize the output intot the form of a vector of
probabilities is known as <em>softmax</em>. Softmax is a function of the form</p>

<script type="math/tex; mode=display">\psi(\bb{z}) =  \frac{ e^{ \bb{z}}  }{
\bb{1}^\Tr e^{ \bb{z}}  }</script>

<p>that highlights the maximal value in the vector $\bb{z}$ and
suppresses other elements that are significantly lower than the maximum.</p>

<h3 id="adding-layers">Adding layers</h3>

<p>The linear perceptron model is rather limited due to its linearity. For example,
it cannot produce the XOR function. A much more powerful family of functions is
obtained by applying a non-linearity to the output of a linear perceptron and
concatenating several such models. We define the $i$-th <em>layer</em> as</p>

<script type="math/tex; mode=display">\bb{y}_i = \varphi_{i} ( \bb{W}_i \bb{y}_{i-1} + \bb{b}_i ),</script>

<p>for $i=1,\dots,L$, where $\bb{y}_{i-1}$ is an
$n_{i-1}$-dimensional input, $\bb{y}_i$ is an
$n_{i}$-dimensional output, $\bb{W}_i$ is an
$n_i \times n_{i-1}$ matrix of weights (whose columns are denoted as
$\bb{w}^{i}_1,\dots, \bb{w}^{i}_{n_{i-1}}$),
$\bb{b}_i$ is an $n_i$-dimensional bias vector, and
$\varphi_i : \RR \rightarrow \RR$ is is a non-linear function applied
element-wise. Setting $\bb{y} = \bb{y}_L$ and
$\bb{y}_0 = \bb{x}$, a <em>multi-layer perceptron</em>
(MLP) with $L$ layers is obtained. MLP can be described by the following
input-to-output map</p>

<script type="math/tex; mode=display">\bb{y}=\varphi_L \left(   \bb{W}_{L} \varphi_{L-1}( \bb{W}_{L-1}  \varphi_{L-2}(  \cdots  \varphi_1(\bb{W}_1 \bb{x})   \cdots  )    ) \right).</script>

<p>parametrized by the weight matrices
${ \bb{W}_1,\dots,\bb{W}_L }$ and bias vectors
${ \bb{b}_1,\dots,\bb{b}_L }$ which we will
collectively denote as a pseudo-vector $\bb{\Theta}$.</p>

<p>Graphically, the $i$-th layer can be thought of a weighted directed graph
connecting each of the $n_{i-1}$ inputs to $n_i$ sum nodes with the
weights given by elements of $\bb{W}_i$. The output of each
sum node undergoes a non-linearity and together the $n_i$ outputs form
the input of the following layer. Because of its (deliberate)
resemblance to biological neural networks, MLP is called an (artificial)
neural network. In the jargon of artificial neural networks, each
sub-graph of the form
$y^i_j = \varphi_i (  \bb{y}_{i-1}^\Tr \bb{w}^{i}_j + b_j  )$
is called a <em>neuron</em> (the $j$-th neuron in $i$-th layer), its
non-linearity $\varphi_i$ is called an <em>activation function</em>, and its
output $y^i_j$ an <em>activation</em>. MLP is a <em>feedforward</em> neural network,
since the graph is acyclic – the data flow forward from the input to the
output without feedback loops.</p>

<p>Unlike their single-layered linear counterparts, MLPs constitute a
potent hypothesis class. In fact, even with just two layers, MLPs were
shown to be <em>universal approximators</em> – their weights can be selected to
approximate any function under mild technical conditions, provided they
have enough degrees of freedom (sufficiently large number of weights).</p>

<h3 id="non-linearity">Non-linearity</h3>

<p>Various functions can be used as the element-wise nonlinearities
(activation function) of the MLP. Older neural networks used the
logistic function (a.k.a. sigmoid)</p>

<script type="math/tex; mode=display">\varphi(t) = \frac{1}{1+e^{-t}}</script>

<p>saturating the input in $\RR$ between $0$ and $1$, or its shifted and
scaled version</p>

<script type="math/tex; mode=display">\varphi(t) =  \frac{e^t  - e^{-t}}{e^t +e^{-t}} = \mathrm{tanh}\, t.</script>

<p>The arctangent function also has a sigmoid-like behavior.</p>

<p>However, due to numerical issues that will be discussed in the sequel, these
functions were nowadays almost universally replaced by the <em>rectifier</em> function
(a.k.a. <em>rectified linear unit</em> or ReLU)</p>

<script type="math/tex; mode=display">\varphi(t) = [t]_+ = \max\{t,0\}.</script>

<p>Note that this function has the derivative of exactly $0$ on $(-\infty,0)$,
exactly $1$ on $(0,\infty)$, and is non-smooth at $0$. These facts justifying
its choice will be discussed in the sequel.</p>

<p>In addition to element-wise non-linearities, modern neural networks sometimes
use “horizontal” non-linearities acting on the entire activation vector. One
typical choice of such a non-linearity adopted in classification networks is a
softmax function applied to the activation of the last (output) layer. Other
non-linearities of this kind are pooling operations that will be discussed in
the sequel.</p>

<h2 id="supervised-training">Supervised training</h2>

<p>Now equipped with a new richer hypothesis class, let us zoom out to see the
whole picture. In the supervised learning problem, we are given a finite sample
of labeled training instances ${  (\bb{x}_i, y_i) }_{i=1}^N$. We
then select a hypothesis that minimizes the empirical (in-sample) loss function,</p>

<script type="math/tex; mode=display">h^\ast =  \mathrm{arg} \min_{h \in \mathcal{H}} \frac{1}{N} \sum_{i=1}^N \ell( h(\bb{x}_i), y_i).</script>

<p>In our terms, this minimization problem can be written as</p>

<script type="math/tex; mode=display">\bb{\Theta}^\ast =  \mathrm{arg} \min_{ \bb{\Theta} }
\frac{1}{N} \sum_{i=1}^N \ell_i ( h_{ \bb{\Theta}}
(\bb{x}_i) ),</script>

<p>where $h_{ \bb{\Theta}}$ is the MLP parametrized by the pseudo-vector
$\bb{\Theta}$. Note that to simplify notation we dropped the
dependence of the $i$-th pointwise loss term on $y_i$, denoting it by $\ell_i$.
We will henceforth denote the loss function as</p>

<script type="math/tex; mode=display">L(\bb{\Theta}) = \frac{1}{N} \sum_{i=1}^N \ell_i ( h_{ \bb{\Theta}} (\bb{x}_i) )</script>

<p>emphasizing that we are interested in its dependence on the model parameters
$\bb{\Theta}$. Let us now discuss how to minimize it with respect to
$\bb{\Theta}$.</p>

<h3 id="global-and-local-minima">Global and local minima</h3>

<p>Let us assume that $L$ is a function of an $m$-dimensional argument
$\bb{\theta}$ defined on all $\RR^m$ (we can always parse all the
degrees of freedom of our neural network into an $m$-dimensional vector). A
point $\bb{\theta}^\ast$ is called a <em>global minimizer</em> of $L$ if for
any $\bb{\theta}$, $L(\bb{\theta}) \ge L(\bb{\theta}^\ast)$.
The corresponding value of the function,
$L(\bb{\theta}^\ast)$, is called a <em>global minimum</em>. The latter term
is often (strictly speaking, erroneously) used to denote the minimizer as well.
A point $\bb{\theta}^\ast$ is called a <em>local minimizer</em> of $L$ if
there exists $\epsilon &gt; 0$ such that $\bb{\theta}^\ast$ is a global
minimizer of $L$ on the ball $B_\epsilon(\bb{\theta}^\ast)$.</p>

<p>Unless $L$ satisfied special properties (such as convexity), finding its global
minimizer is an unsolvable problem. On the other hand, finding a local minimizer
is a much easier task, since local minimizers can be characterized using local
information (i.e., derivatives). Assuming $L$ is $\mathcal{C}^1$, from
elementary multivariate calculus we should recollect the first-order necessary
condition for $\bb{\theta}^\ast$ being a local minimizer:</p>

<script type="math/tex; mode=display">\nabla_{ \bb{\theta}} L(\bb{\theta}^\ast) = \bb{0}.</script>

<p>Obviously, this is not a sufficient condition – in fact, a local maximum and a
saddle point also satisfy it. However, the latter two types of extremal points
(characterized by negative curvature) are <em>unstable</em>, which will allow methods
such as stochastic gradient descent not to remain stuck at such points.</p>

<p>As a reminder, the <em>gradient</em> of a multi-variate function is an operator
$\nabla L : \RR^m \rightarrow \RR^m$. At a given point
$\bb{\theta}$, it produces a vector
$\bb{g} = \nabla L(\bb{\theta})$ satisfying</p>

<script type="math/tex; mode=display">dL = \langle \bb{g}, \bb{dx} \rangle = \bb{g}^\Tr \bb{d\theta};</script>

<p>in other words, an inner product of the argument change
$\bb{d\theta}$ with the gradient yields the differential $dL$.</p>

<h3 id="gradient-descent">Gradient descent</h3>

<p>We can therefore suggest a very simple iterative strategy for finding a local
minimum, which can be summarized as the following “algorithm”:</p>

<p>Starting with some <em>initial guess</em> $\bb{\theta}_0$, repeat for
$k=1,2,\dots$</p>

<ol>
  <li>Select a <em>descent direction</em> $\bb{d}_k$</li>
  <li>Select a <em>step size</em> $\eta_k$</li>
  <li>Update
$\bb{\theta}_k = \bb{\theta}_{k-1} + \eta_k \bb{d}_k$</li>
  <li>Check optimality condition at $\bb{\theta}_k$ and stop if
minimum is reached</li>
</ol>

<p>(In practice, rather than checking the optimality condition, we will run the
algorithm for a fixed number of iterations and stop it prematurely based on the
value of cross-validation loss – these details will be discussed further in the
course.)</p>

<p>The main ingredient of the above “algorithm” is the choice of the descent
direction, i.e., a direction a (small) step in which decreases the value of the
function. Let $\bb{\theta}$ be our current iterate (we drop the
iteration subscript) and let $\bb{d}$ be a direction. Once a
direction is choses, we can consider a one-dimensional “section” of the
$m$-dimensional function $L$,</p>

<script type="math/tex; mode=display">f(\eta) = L(\bb{\theta}+\eta \bb{d}).</script>

<p>The quantity</p>

<script type="math/tex; mode=display">f'(0) = \left. \frac{d L(\bb{\theta}+\eta \bb{d})
}{d\eta} \right|_{\eta = 0} = \nabla L(\bb{\theta})^\Tr
\bb{d}</script>

<p>is known as the <em>directional derivative</em> of $L$ at point
$\bb{\theta}$ in the direction $\bb{d}$. A negative
directional derivative indicates that a small step in the direction
$\bb{d}$ decreases the value of the function.  Geometrically, this
means that a descent direction forms an <em>obtuse angle</em> with the gradient (or an
acute angle with the negative gradient).</p>

<p>Let us now approximate our function linearly around
$\bb{\theta}$,</p>

<script type="math/tex; mode=display">L(\bb{\theta}+\bb{d}) \approx L(\bb{\theta}) +
\nabla L(\bb{\theta})^\Tr \bb{d}</script>

<p>and ask ourselves what direction minimizes the difference
$L(\bb{\theta}+\bb{d}) -  L(\bb{\theta})
\approx \nabla L(\bb{\theta})^\Tr \bb{d}$ – we could call
such a direction the <em>steepest</em> descent direction.  Obviously, this linear
approximation is unbounded, so we need to normalize the length of
$\bb{d}$. Different choices of the norm lead to different answers (so
there are many steepest directions); in the $\ell_2$ sense we obtain</p>

<script type="math/tex; mode=display">\bb{d} = -\nabla \bb{d}.</script>

<p>This choice of the descent direction leads to a family of algorithms known as
<em>gradient descent</em>.</p>

<p>Our next goal is to select the step size $\eta$. Ideally, once we have the
direction $\bb{d}$, we would like to solve for</p>

<script type="math/tex; mode=display">\eta = \mathrm{arg}\min_{\eta} L(\bb{\theta}+\eta \bb{d}).</script>

<p>While there exist various methods known as <em>line search</em> to solve such a
one-dimensional minimization problem, usually they come at the expense of
unaffordable extra complexity. In deep learning, a much more common choice is to
use a vanishing sequence of weights that start with some initial $\eta_0$ which
is kept for a certain number of iterations and then gradually reduced as $1/k$.
Using the statistical mechanics metaphor, such a reduction in the step size
resembles a decrease in temperature and is therefore referred to as <em>annealing</em>.</p>

<p>Gradient descent can be thus summarized as</p>

<p>Starting with some <em>initial guess</em> $\bb{\theta}_0$, repeat for
$k=1,2,\dots$</p>

<ol>
  <li>Select a <em>step size</em> $\eta_k$</li>
  <li>Update
$\bb{\theta}_k = \bb{\theta}_{k-1} - \eta_k \nabla L(\bb{\theta}_{k-1})$</li>
  <li>Check optimality condition at $\bb{\theta}_k$ and stop if
minimum is reached</li>
</ol>

<p>We will discuss variants of the gradient descent algorithm that are used
in practice in the sequel.</p>

<h3 id="error-backpropagation">Error backpropagation</h3>

<p>The main computation ingredient in the gradient descent algorithm is the
gradient of the loss function w.r.t. the network parameters
$\bb{\theta}$. Obviously, since an MLP is just a composition of
multi-variate functions, the gradient can be simply computed invoking the chain
rule. However, recall that the output of the network is usually a
$k$-dimensional vector, whereas the parameters are a collection of
$n_i \times n_{i-1}$ weight matrices and $n_i$-dimensional bias vectors. The
gradient of a vector with respect to a matrix (formally termed the Jacobian) is
a third-order tensor, which is not exactly nice to work with.</p>

<p>A much more elegant approach to apply the chain rule takes advantage of the
layered structure of the network. As an illustration, we start with a two-layer
MLP of the form</p>

<script type="math/tex; mode=display">\bb{y} = \varphi( \bb{A}  \phi(\bb{B} \bb{x} ) ),</script>

<p>where $\varphi$ and $\phi$ are the two non-linearities, and
$\bb{A}$ and $\bb{B}$ are the two weight matrices.
We are ignoring the bias terms for the sake of exposition clarity. To
analyze the influence of the last (second) layer, we denote its input as
$\bb{y}’ =  \phi(\bb{B} \bb{x} )$, and
the input to the second layer activation function as
$\bb{z} = \bb{A}\bb{y}’$. In this
notation, we have
$\bb{y} = \varphi(\bb{A} \bb{y}’)$.
According to the chain rule,</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial \bb{A}} =
\frac{\partial \bb{y} }{\partial \bb{A}} \frac{\partial L}{\partial \bb{y}} =
\sum_{j=1}^k \frac{\partial y_j }{\partial \bb{A}} \frac{\partial L}{\partial y_j}.</script>

<p>For convenience, let us adopt the standard deep learning notation,
according to which the derivative of the loss w.r.t. to a parameter
$\bb{*}$ is denoted as $\delta \bb{\ast}$. In our
case,</p>

<script type="math/tex; mode=display">\delta \bb{y} = \frac{\partial L}{\partial \bb{y}} =
\left( \frac{\partial L}{\partial y_1},\dots,  \frac{\partial L}{\partial y_k} \right)^\Tr</script>

<p>is the gradient of the loss w.r.t. its input, and
$\delta \bb{A}$ is a matrix whose elements are
$\frac{\partial L}{\partial a_{ij} }$, etc. In this notation, we can
rewrite</p>

<script type="math/tex; mode=display">\delta \bb{A} =
\sum_{j=1}^k \frac{\partial y_j }{\partial \bb{A}} \, \delta  y_j.</script>

<p>We can write $\frac{\partial y_j }{\partial \bb{A}}$ as a
matrix of the size of $\bb{A}$, filled with zeros except the
$j$-th row, which is given by
$\varphi’(z_j) \bb{y}^{\prime \Tr}$. Substituting this result
into the former sum yields</p>

<script type="math/tex; mode=display">\delta \bb{A} =
\left(
    \begin{array}{c}
        \delta y_1 \varphi'(z_1)  \bb{y}^{\prime \Tr} \\ \vdots \\ \delta y_k \varphi'(z_k)
        \bb{y}^{\prime \Tr}
    \end{array}
\right) =
\mathrm{diag}\{ \delta \bb{y} \}  \, \mathrm{diag}\{ \varphi'(\bb{z}) \} 
\left(
    \begin{array}{c}
        \bb{y}^{\prime \Tr} \\
        \vdots \\
        \bb{y}^{\prime \Tr}
    \end{array}
\right) =
\mathrm{diag}\{ \delta \bb{y} \}  \, \mathrm{diag}\{
\varphi'(\bb{z}) \} \bb{1} \bb{y}^{\prime \Tr}.</script>

<p>To analyze the influence of the first layer, we denote $\bb{z}’ =
\bb{B}\bb{x}$. To derive the gradient of the loss w.r.t.
the first layer parameter $\bb{B}$, we again invoke the chain rule</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial \bb{B}} = \frac{\partial
\bb{y}' }{\partial \bb{B}} \frac{\partial L}{\partial
\bb{y}'} = \frac{\partial \bb{y}' }{\partial
\bb{B}} \frac{\partial \bb{y} }{\partial
\bb{y}'}  \frac{\partial L}{\partial \bb{y}} = \sum_{j=1}
\frac{\partial y'_j }{\partial \bb{B}} \, \delta y'_j.</script>

<p>As before, $\frac{\partial y’_j }{\partial \bb{B}}$ is a matrix of
the size of $\bb{B}$, filled with zeros except the $j$-th row, which
is given by $\phi’(z’_j) \bb{x}^\Tr$, so</p>

<script type="math/tex; mode=display">\delta \bb{B} =
\mathrm{diag}\{ \delta \bb{y}' \}  \, \mathrm{diag}\{
\phi'(\bb{z}') \}  \bb{1} \bb{x}^\Tr.</script>

<p>It remains to derive</p>

<script type="math/tex; mode=display">\partial \bb{y}' =
\frac{\partial L}{\partial \bb{y}'} =
\frac{\partial \bb{y} }{\partial \bb{y}'}
\frac{\partial L}{\partial \bb{y}}.</script>

<p>From $\bb{y} = \varphi(\bb{A} \bb{y}’)$, we
have</p>

<script type="math/tex; mode=display">\frac{\partial \bb{y} }{\partial \bb{y}'} = \diag\{\varphi'(\bb{z} )  \} \bb{A}^\Tr,</script>

<p>from where</p>

<script type="math/tex; mode=display">\delta \bb{y}' =   \diag\{ \varphi'(\bb{z} )  \} \bb{A}^\Tr \delta \bb{y}.</script>

<p>We can therefore summarize the chain rule in our two-layer MLP as
follows: First, we propagate the data <em>forward</em> through the network,
computing</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\bb{z}' &=& \bb{B}\bb{x}  \\
\bb{y}' &=& \phi( \bb{z}' ) \\
\bb{z} &=& \bb{A}\bb{y}'  \\
\bb{y} &=& \varphi( \bb{z} ).
\end{aligned} %]]></script>

<p>Then,
we propagate the derivatives <em>backward</em> through the network:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\delta \bb{y} &=& \nabla L( \bb{y} ) \\
\delta \bb{A} &=&  \mathrm{diag}\{ \delta \bb{y} \}  \, \mathrm{diag}\{ \varphi'(\bb{z}) \}   \bb{1} \bb{y}^{\prime \Tr} \\
\delta \bb{y}' &=&   \diag\{ \varphi'(\bb{z} )  \} \bb{A}^\Tr \delta \bb{y} \\
\delta \bb{B} &=&  \mathrm{diag}\{ \delta \bb{y}' \}  \, \mathrm{diag}\{ \phi'(\bb{z}') \}   \bb{1} \bb{x}^\Tr.
\end{aligned} %]]></script>

<p>The entire procedure, known as error backward propagation or
<em>backpropagation</em> for short can be applied recursively for any number of
layers.</p>

<h4 id="forward-pass">Forward pass:</h4>

<p>Starting with $\bb{y}_0 = \bb{x}$, compute for
$k=1,\dots, L$</p>

<ul>
  <li>
    <p>$\bb{z}_k = \bb{W}_k \bb{y}_{k-1}$</p>
  </li>
  <li>
    <p>$\bb{y}_k = \varphi_k(\bb{z}_k)$</p>
  </li>
</ul>

<p>and output $\bb{y} = \bb{y}_L$.</p>

<h4 id="backward-pass">Backward pass:</h4>

<p>Starting with $\delta {y}_L = \nabla L( \bb{y} )$, compute
for $k=L,L-1,\dots, 1$</p>

<ul>
  <li>
    <p>$\delta \bb{W}_k =  \mathrm{diag}{ \delta \bb{y}_k } \, \mathrm{diag}{ \varphi’_k (\bb{z}_k) } \bb{1} \bb{y}_{k-1}^\Tr$</p>
  </li>
  <li>
    <p>$\delta \bb{b}_k =  \mathrm{diag}{ \delta \bb{y}_k }
\varphi’_k (\bb{z}_k)$</p>
  </li>
  <li>
    <p>$\delta \bb{y}_{k-1} =   \diag{ \varphi’_k(\bb{z_k} ) }
\bb{W}_k^\Tr \delta \bb{y}_k$</p>
  </li>
</ul>

<p>We remind that $\delta \bb{W}_k$ and
$\delta \bb{b}_k$ are blocks of coordinates of the gradient
of the loss $L$ with respect to the network parameters.</p>

<h3 id="exploding-and-vanishing-gradients">Exploding and vanishing gradients</h3>

<p>Backpropagation allows a recursive calculation of the loss gradient w.r.t. the
parameters of the network without the need to ever construct the Jacobian
matrices of each layer’s output w.r.t. its input. Note, however, that in order
to compute the gradient w.r.t. the first layer, $\delta \bb{W}_1$,
one need to compute the product of $\varphi’_L
(\bb{z}_L),\dots,\varphi’_1 (\bb{z}_1)$.  This may lead to
numerical instabilities. For example, in a network with $L=20$ layers, a slopeof
$\varphi’ = 2$ in each activation function would be amplified by $10^6$.
Similarly, a slope of $\varphi’ = 0.5$ would diminish to $10^{-6}$ – practically
to zero. This problem is known as vanishing and exploding gradients, and it
prevented end-to-end supervised training of deep neural networks from random
initialization.</p>

<p>The introduction of ReLU activations mitigated this problem. In ReLU, the
derivative is $1$ for positive arguments and $0$ for negative ones.  This
implies that depending on the path through the network from the output back to
the inputs, the product of the activation derivatives will always be either $0$
or $1$. The $0$ derivative for negative arguments could still lead to vanishing
gradients, but practice shows that, on the contrary, it helps optimization and
promotes sparse solutions.</p>

<p>ReLU was probably one of the few significant algorithmic changes in the
classical neural networks that enabled deep learning.</p>

<h2 id="convolutional-neural-networks">Convolutional neural networks</h2>

<p>The layers on MLP described so far are termed <em>fully connected</em> in the deep
learning literature, due to the fact that every layer input is connected
(through some weight) to every output. For large input and output dimensions,
such an architecture results in a vast number of degrees of freedom, which
increases the network complexity and requires more data to train.</p>

<h3 id="weight-sharing-and-shift-invariance">Weight sharing and shift invariance</h3>

<p><em>Weight sharing</em> is a strategy aiming at reducing the layer complexity by
reusing the same weights at different parts of the input. For the sake of the
following discussion, we assume the input to be discrete and infinitely
supported (i.e., a sequence $\bb{x} = { x_i },~{i \in \mathbb{Z}}$).
The output is also assumed to be a sequence,
$\bb{y} = { y_i },~{i \in \mathbb{Z}}$.
Let us consider the output of the $i$-th neuron,</p>

<script type="math/tex; mode=display">y_i = \varphi\left( \sum_{j \in \mathbb{Z}} w_{ij} x_j + b+i \right).</script>

<p>In many cases such as audio signals, images, etc., it is reasonable to assume
that the same operation is valid at different parts of the signal.
Mathematically, this can be expressed by asserting that the action of the neuron
commutes with the action of a translation group.  This leads to demanding</p>

<script type="math/tex; mode=display">\varphi\left( \sum_{j \in \mathbb{Z}} w_{i-m,j} x_j + b_{i-m} \right) =
\varphi\left( \sum_{j \in \mathbb{Z}} w_{ij} x_{j-m} + b_i \right)</script>

<p>for every input $\bb{x}$. Since the non-linearity is applied
element-wise, the equivalent condition holds on its arguments as well,</p>

<script type="math/tex; mode=display">\sum_{j \in \mathbb{Z}} w_{i-m,j} x_j  + b_{i-m} =
\sum_{j \in \mathbb{Z}} w_{ij} x_{j-m} + b_i =
\sum_{j' \in \mathbb{Z}} w_{i,j'+m} x_{j'} + b_i.</script>

<p>This implies $b_i = \mathrm{const}$ and $w_{i-m,j} = w_{i,j+m}$; in
other words, if we consider $w_{ij}$ to be the elements of an inifinite
weight matrix, it will have equal elements on each of its diagonals.
Another way to express is is by saying that $w_{ij}$ is a function of
$i-j$.</p>

<h3 id="toeplitz-operators-and-convolution">Toeplitz operators and convolution</h3>

<p>A linear operator exhibiting the above structure is called <em>Toeplitz</em>.  The
output of a shift-invariant (Toeplitz) neuron can be written as</p>

<script type="math/tex; mode=display">y_i = \varphi\left( \sum_{j \in \mathbb{Z}} w_{i-j} x_j + b \right).</script>

<p>Note that the weights $\bb{w}$ can now be considered as a window that
is applied to the input at a certain location to produce an output at the same
location, and then is slided to a different input location to produce the
corresponding output. This operation (the application of the Toeplitz operator)
called <em>convolution</em>, denoted as</p>

<script type="math/tex; mode=display">(w \ast x)_i =
\sum_{j \in \mathbb{Z}} w_{i-j} x_j =
\sum_{j \in \mathbb{Z}} w_j x_{i-j} =
(x \ast w)_i.</script>

<p>In this notation, the action of our layer can be written as</p>

<script type="math/tex; mode=display">\bb{y} =
\varphi\left( \bb{w} \ast \bb{x} + b \right).</script>

<p>In the signal processing jargon, we can say that the input signal
$\bb{x}$ is filtered by a filter with the impulse response
$\bb{w}$.</p>

<h3 id="convolutional-layer">Convolutional layer</h3>

<p>Neural networks making use of shift-invariant linear operations are called
<em>convolutional neural networks</em> (CNNs). A convolutional layer accepts an
$m$-dimensional <em>vector-valued</em> infinitely supported signal
$\bb{x} = (\bb{x}^1,\dots, \bb{x}^m) =
{ (x_i^1,\dots, x_i^m) }_{i \in \mathbb{Z}}$;
each input dimension is called a <em>channel</em> or <em>feature map</em>.
The layer produces an $n$-dimensional infinitely supported signal
$\bb{y} = (\bb{y}^1,\dots, \bb{y}^n) =
{ (y_i^1,\dots, y_i^n) }_{i \in \mathbb{Z}}$ by applying a bank of filters,</p>

<script type="math/tex; mode=display">\bb{y}^j =
\varphi\left(  \sum_{i=1}^m \bb{w}^{ij} \ast \bb{x}^{i}  \right),</script>

<p>or, explicitly,</p>

<script type="math/tex; mode=display">y^j_k = \varphi\left(  \sum_{i=1}^m \sum_{p} w^{ij}_p x^i_{k-p}  \right).</script>

<p>In practice, each filter $w^{ij}$ is supported on some small fixed
domain.</p>


        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/cs236781/semesters/sp19/lecture_notes/lecture_02/" class="pagination--pager" title="Lecture 2: Supervised Learning
">Previous</a>
    
    
      <a href="/cs236781/semesters/sp19/lecture_notes/lecture_05/" class="pagination--pager" title="Lecture 5: Recurrent Neural Networks
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    <div class="page__footer">
      <footer>
        
<!-- Technion and VISTA logos --><script>

var logo_element = '\
<div class="technion-logo"> \
    <a href="https://cs.technion.ac.il"> \
        <img src="/cs236781/semesters/sp19/assets/images/cs_technion-logo.png" alt="Technion"> \
    </a> \
</div> \
';

document
    .querySelector('.masthead__inner-wrap')
    .insertAdjacentHTML('afterbegin', logo_element);

var logo_element = '\
<div class="vista-logo"> \
    <a href="https://vista.cs.technion.ac.il" > \
        <img src="/cs236781/semesters/sp19/assets/images/vista-logo-bw.png" alt="VISTA"> \
    </a> \
</div> \
';

var footerNodes = document.getElementsByTagName("FOOTER")
var footerNode = footerNodes[footerNodes.length - 1];
footerNode.insertAdjacentHTML('afterend', logo_element);

</script>
<!-- Mathjax support --><!-- see: http://haixing-hu.github.io/programming/2013/09/20/how-to-use-mathjax-in-jekyll-generated-github-pages/ -->
<!-- also: http://docs.mathjax.org/en/latest/tex.html for defning mathjax macros -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noErrors: { disabled: true },
      Macros: {
        // Each def here is an array: [<macro>, <num_params>]
        // Aviv's defs
        bold: ["{\\bf #1}",1],
        m: ["\\boldsymbol {#1}",1],             // matrix
        mt: ["\\boldsymbol {#1}^\\top",1],      // transposed matrix
        v: ["\\boldsymbol {#1}",1],             // vector
        vt: ["\\boldsymbol {#1}^\\top",1],      // transposed vector
        diag: ["\\mathop{\\mathrm{diag}}"],
        trace: ["\\mathop{\\mathrm{tr}}"],
        rank: ["\\mathop{\\mathrm{rank}}"],
        set: ["\\mathbb {#1}",1],
        rvar: ["\\mathrm{#1}",1],               // random variable
        rvec: ["\\boldsymbol{\\mathrm{#1}}",1], // random vector

        // Alex's defs
        bm: ["{\\bf #1}",1],
        bb: ["{\\bm{\\mathrm{#1}}}",1],
        spn: ["\\mathrm{span}\\left\\{ {#1} \\right\\}",1],
        vec: ["\\mathrm{vec}"],
        dx:  ["\\bb{dx}"], dX:  ["\\bb{dX}"], dy:  ["\\bb{dy}"], du:  ["\\bb{du}"],
        df:  ["\\bb{df}"], dg:  ["\\bb{dg}"],
        dphi:  ["\\bb{d\\varphi}"],
        Tr: ["\\top"],
        RR: ["\\set{R}"],
        mathpzc: ["\\rvar{#1}", 1],
        mathpzcb: ["\\rvec{#1}", 1],
        ind: ["\\unicode{x1D7D9}"]
      }
    },

  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$','$$'] ],
    processEscapes: true,
  },

  "HTML-CSS": {
     fonts: ["TeX"]
  },

});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- Copyright notice support on single pages -->
<script>
var copyright_element = '\
    <p class="page__meta" style="margin-top: -0.5em;"> \
    <i class="far fa-copyright"></i> \
    Prof. Alex Bronstein \
    </p> \
';

first_header = document.getElementsByTagName('header')[0]
first_header.insertAdjacentHTML('beforeend', copyright_element);
</script>


        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://vista.cs.technion.ac.il"><i class="fas fa-fw fa-link" aria-hidden="true"></i> VISTA Lab</a></li>
        
      
        
          <li><a href="https://github.com/vistalab-technion"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/cs236781/semesters/sp19/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 VISTA Lab. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/cs236781/semesters/sp19/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>




<script src="/cs236781/semesters/sp19/assets/js/lunr/lunr.min.js"></script>
<script src="/cs236781/semesters/sp19/assets/js/lunr/lunr-store.js"></script>
<script src="/cs236781/semesters/sp19/assets/js/lunr/lunr-en.js"></script>





  </body>
</html>
